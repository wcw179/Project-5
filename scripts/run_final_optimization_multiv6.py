"""
Comprehensive script (v6) for hyperparameter optimization using Optuna.

This version incorporates several key enhancements for robust financial ML modeling:
1.  **PurgedStratifiedKFold:** Uses a sophisticated cross-validator to prevent data
    leakage and preserve class balance, crucial for time-series data.
2.  **Configurable Financial Metrics:** Allows switching between proxy returns and
    true returns (`ret` column) for calculating Sharpe, Sortino, and Hit Ratio.
3.  **Flexible Combined Metric:** The primary optimization metric is a weighted
    combination of accuracy and a financial metric, with weights configurable
    in the `CONFIG` dictionary.
4.  **Detailed Results Storage:** Saves not only the best trial but also detailed
    metrics for each fold of each trial to a CSV for in-depth analysis.
5.  **Safe Column Handling:** Robustly handles the presence or absence of a
    'symbol' column in the feature set.
"""
import sys
from pathlib import Path
import json
import joblib
import numpy as np
import optuna
from optuna.samplers import TPESampler
import pandas as pd
import xgboost as xgb
import gc
from loguru import logger
from sklearn.metrics import f1_score, classification_report, accuracy_score
from typing import Dict, List, Tuple, Any, Optional
import warnings
warnings.filterwarnings('ignore')

# Add project root to the Python path
project_root = Path(__file__).resolve().parent.parent
sys.path.append(str(project_root))

# Import our enhanced cross-validation
from src.model_validation.financial_ml_cross_validation_v3 import PurgedStratifiedKFold

# --- Logger Configuration ---
log_file_path = project_root / "logs" / "enhanced_optimization_multi_v6.log"
log_file_path.parent.mkdir(parents=True, exist_ok=True)
logger.remove()
logger.add(sys.stdout, colorize=True, format="<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <cyan>{name}:{function}:{line}</cyan> - <level>{message}</level>")
logger.add(log_file_path, rotation="10 MB", retention="10 days", enqueue=True, serialize=False)

# --- Enhanced Configuration ---
MODEL_DIR = project_root / "models"
CONFIG = {
    'n_trials': 100,
    'n_splits': 5,
    'random_state': 42,
    'n_jobs_optuna': 1,
    'n_jobs_xgb': -1,
    'xgb_early_stopping': 50,
    'pct_embargo': 0.01,
    'min_samples_per_class': 10,
    'use_true_returns': True,
    'annualization_factor': 252 * 288, # 252 trading days, 288 5-min bars per day
    'evaluation_metrics': ['f1_weighted', 'accuracy', 'sharpe', 'sortino', 'hit_ratio', 'combined'],
    'primary_metric': 'combined',
    'metric_weights': {
        'accuracy': 0.2,
        'sharpe': 0.8
    },
    # Config for normalizing Sharpe ratio into a [0, 1] range for the combined metric.
    'sharpe_normalization': {
        'method': 'tanh',   # 'tanh' or 'linear'
        'scaling_factor': 2.0, # For tanh: higher value flattens the curve, sensitive in [-2, 2]
        'offset': 2,   # For linear fallback
        'scale': 5     # For linear fallback
    }
}


def evaluate_predictions(y_true: pd.Series, y_pred: np.array,
                         true_returns: Optional[pd.Series] = None) -> Dict[str, float]:
    """Calculate comprehensive evaluation metrics using either true returns or a proxy."""
    metrics = {}
    annualization_factor = CONFIG['annualization_factor']

    # --- Classification Metrics ---
    metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted')
    metrics['accuracy'] = accuracy_score(y_true, y_pred)

    # --- Financial Metrics ---
    positions = pd.Series(y_pred, index=y_true.index).map({1: 1, -1: -1, 0: 0})

    if CONFIG['use_true_returns'] and true_returns is not None:
        strategy_returns = positions * true_returns
    else:
        # Fallback to proxy returns if true returns are not available or configured off
        market_proxy = y_true.map({1: 0.001, -1: -0.001, 0: 0})
        strategy_returns = positions * market_proxy

    strategy_returns.fillna(0, inplace=True)
    active_trades = strategy_returns[strategy_returns != 0]

    # Sharpe Ratio
    if np.std(strategy_returns) > 0:
        metrics['sharpe'] = (np.mean(strategy_returns) / np.std(strategy_returns)) * np.sqrt(annualization_factor)
    else:
        metrics['sharpe'] = 0.0

    # Sortino Ratio
    downside_returns = strategy_returns[strategy_returns < 0]
    downside_std = np.std(downside_returns) if len(downside_returns) > 1 else 0
    if downside_std > 0:
        metrics['sortino'] = (np.mean(strategy_returns) / downside_std) * np.sqrt(annualization_factor)
    else:
        metrics['sortino'] = 0.0

    # Hit Ratio
    if not active_trades.empty:
        metrics['hit_ratio'] = (active_trades > 0).sum() / len(active_trades)
    else:
        metrics['hit_ratio'] = np.nan # Use np.nan for folds with no trades

    # --- Combined Metric (from CONFIG) ---
    weights = CONFIG['metric_weights']
    norm_config = CONFIG['sharpe_normalization']
    # Tanh normalization provides a smooth, bounded compression of Sharpe ratios.
    # It's less sensitive to extreme outliers than linear scaling.
    # Test cases: Sharpe=0.5 -> ~0.62, Sharpe=1.0 -> ~0.73, Sharpe=3.0 -> ~0.98
    if norm_config.get('method') == 'tanh':
        scaling = norm_config.get('scaling_factor', 2.0)
        normalized_sharpe = (np.tanh(metrics['sharpe'] / scaling) + 1) / 2
    else: # Fallback to linear normalization
        offset = norm_config.get('offset', 2)
        scale = norm_config.get('scale', 5)
        normalized_sharpe = max(0, min(1, (metrics['sharpe'] + offset) / scale))
    metrics['combined'] = (weights['accuracy'] * metrics['accuracy'] +
                         weights['sharpe'] * normalized_sharpe)

    return metrics


def objective(trial: optuna.trial.Trial, X: pd.DataFrame, y_df: pd.DataFrame,
              cv_splits: List[Tuple[np.array, np.array]]) -> float:
    """Enhanced Optuna objective function with comprehensive evaluation."""
    try:
        params = {
            'objective': 'multi:softprob',
            'num_class': 3,
            'eval_metric': 'mlogloss',
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'gamma': trial.suggest_float('gamma', 0, 5),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 2.0, log=True),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 2.0, log=True),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
            'tree_method': 'hist',
            'random_state': CONFIG['random_state'],
            'n_jobs': CONFIG['n_jobs_xgb'],
        }

        fold_metrics = {metric: [] for metric in CONFIG['evaluation_metrics']}
        valid_folds = 0

        for fold, (train_idx, val_idx) in enumerate(cv_splits):
            try:
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train_df, y_val_df = y_df.iloc[train_idx], y_df.iloc[val_idx]
                y_train, y_val = y_train_df['label'], y_val_df['label']

                X_train_numeric = X_train.drop(columns=['symbol'], errors='ignore')
                X_val_numeric = X_val.drop(columns=['symbol'], errors='ignore')

                y_train_mapped = y_train.map({-1: 0, 0: 1, 1: 2})
                y_val_mapped = y_val.map({-1: 0, 0: 1, 1: 2})

                if len(np.unique(y_train_mapped)) < 2: continue

                model = xgb.XGBClassifier(**params)
                model.fit(X_train_numeric, y_train_mapped, eval_set=[(X_val_numeric, y_val_mapped)],
                          early_stopping_rounds=CONFIG['xgb_early_stopping'], verbose=False)

                preds = np.argmax(model.predict_proba(X_val_numeric), axis=1)
                preds_mapped_back = pd.Series(preds).map({0: -1, 1: 0, 2: 1}).values

                metrics = evaluate_predictions(y_val, preds_mapped_back, true_returns=y_val_df['ret'])
                for name, score in metrics.items(): fold_metrics[name].append(score)
                valid_folds += 1
                del model, preds, preds_mapped_back
            except Exception as e:
                logger.error(f"Error in fold {fold}: {e}")
                continue

        # Call gc.collect() once per trial, after all folds are complete.
        gc.collect()

        if valid_folds == 0: return -1.0

        mean_scores = {metric: np.nanmean(scores) for metric, scores in fold_metrics.items()}
        trial.set_user_attr("fold_metrics", fold_metrics)
        for metric, score in mean_scores.items():
            trial.set_user_attr(f"mean_{metric}", score)
        
        primary_score = mean_scores.get(CONFIG['primary_metric'], -1.0)
        return float(primary_score)

    except Exception as e:
        logger.error(f"Trial {trial.number} failed entirely: {e}")
        return -1.0

def save_comprehensive_results(study: optuna.study.Study, model_dir: Path):
    """Save comprehensive optimization results, handling partial results and failed trials."""
    logger.info("Saving comprehensive optimization results...")
    model_dir.mkdir(parents=True, exist_ok=True)

    # --- Save Fold Metrics (Robustly) ---
    all_fold_metrics = []
    for trial in study.trials:
        if 'fold_metrics' in trial.user_attrs:
            fm = trial.user_attrs['fold_metrics']
            num_folds = len(next(iter(fm.values()), []))
            for i in range(num_folds):
                res = {'trial': trial.number, 'fold': i}
                for metric, scores in fm.items():
                    if i < len(scores):
                        res[metric] = scores[i]
                all_fold_metrics.append(res)

    if all_fold_metrics:
        fold_metrics_df = pd.DataFrame(all_fold_metrics)
        fold_metrics_df.to_csv(model_dir / "fold_metrics_v6.csv", index=False)
        logger.success(f"Fold metrics saved to {model_dir / 'fold_metrics_v6.csv'}")

    # --- Save Standard Results ---
    joblib.dump(study, model_dir / "optimization_study_v6.pkl")
    study.trials_dataframe().to_csv(model_dir / "optimization_trials_v6.csv", index=False)

    if study.best_trial:
        best_trial_results = {
            'best_value': study.best_value,
            'best_params': study.best_params,
            'best_trial_number': study.best_trial.number,
            'optimization_config': CONFIG
        }
        with open(model_dir / "best_trial_results_v6.json", "w") as f:
            json.dump(best_trial_results, f, indent=4, default=str)

    logger.success("All optimization results saved.")

def final_model_validation(study: optuna.study.Study, X: pd.DataFrame, y_df: pd.DataFrame,
                          cv_splits: List[Tuple[np.array, np.array]]) -> Dict[str, Any]:
    """Comprehensive final model validation with all metrics"""
    logger.info("=== Starting Comprehensive Final Model Validation ===")

    # Prepare best parameters
    best_params = study.best_params.copy()
    best_params.update({
        'objective': 'multi:softprob',
        'num_class': 3,
        'eval_metric': 'mlogloss',
        'tree_method': 'hist',
        'random_state': CONFIG['random_state'],
        'n_jobs': -1,
    })

    # Cross-validation results storage
    all_predictions = []
    all_true_labels = []
    all_val_indices = [] # Store original indices to prevent mismatch
    fold_results = []
    saved_models = []

    for fold, (train_idx, val_idx) in enumerate(cv_splits):
        logger.info(f"Validating fold {fold + 1}/{len(cv_splits)}")

        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train_df, y_val_df = y_df.iloc[train_idx], y_df.iloc[val_idx]
        y_train, y_val = y_train_df['label'], y_val_df['label']

        X_train_numeric = X_train.drop(columns=['symbol'], errors='ignore')
        X_val_numeric = X_val.drop(columns=['symbol'], errors='ignore')

        y_train_mapped = y_train.map({-1: 0, 0: 1, 1: 2})
        y_val_mapped = y_val.map({-1: 0, 0: 1, 1: 2})

        model = xgb.XGBClassifier(**best_params)
        model.fit(X_train_numeric, y_train_mapped, eval_set=[(X_val_numeric, y_val_mapped)],
                  early_stopping_rounds=CONFIG['xgb_early_stopping'], verbose=False)

        preds = np.argmax(model.predict_proba(X_val_numeric), axis=1)
        preds_mapped_back = pd.Series(preds).map({0: -1, 1: 0, 2: 1}).values

        # Store predictions, labels, and original indices for correct alignment later
        all_predictions.extend(preds_mapped_back)
        all_true_labels.extend(y_val.values)
        all_val_indices.extend(y_val.index)

        fold_metrics = evaluate_predictions(y_val, preds_mapped_back, true_returns=y_val_df['ret'])
        fold_results.append(fold_metrics)

        model_path = MODEL_DIR / f"final_model_fold_{fold}.joblib"
        joblib.dump(model, model_path)
        saved_models.append(str(model_path))

        logger.info(f"Fold {fold} metrics: " + ", ".join([f"{k}: {v:.4f}" for k, v in fold_metrics.items()]))

    # --- Aggregate results with correct index alignment ---
    # Reconstruct Series with the original validation indices to prevent misalignment
    all_true_labels_s = pd.Series(all_true_labels, index=all_val_indices)
    all_predictions_s = pd.Series(all_predictions, index=all_val_indices)
    all_true_returns_s = y_df.loc[all_val_indices, 'ret']

    overall_metrics = evaluate_predictions(
        all_true_labels_s,
        all_predictions_s.values,
        true_returns=all_true_returns_s
    )

    # Calculate cross-validation statistics
    cv_stats = {}
    for metric in CONFIG['evaluation_metrics']:
        scores = [fold[metric] for fold in fold_results if metric in fold]
        if scores:
            cv_stats[metric] = {
                'mean': np.nanmean(scores),
                'std': np.std(scores),
                'scores': scores
            }

    # Classification report
    class_report = classification_report(
        all_true_labels, all_predictions,
        output_dict=True, digits=4
    )

    logger.info("=== Final Validation Results ===")
    logger.info("Overall Metrics:")
    for metric, score in overall_metrics.items():
        logger.info(f"  {metric}: {score:.4f}")

    logger.info("\nCross-Validation Statistics:")
    for metric, stats in cv_stats.items():
        logger.info(f"  {metric}: {stats['mean']:.4f} Â± {stats['std']:.4f}")

    logger.info(f"\nClassification Report:\n{classification_report(all_true_labels, all_predictions, digits=4)}")

    return {
        'overall_metrics': overall_metrics,
        'cv_statistics': cv_stats,
        'fold_results': fold_results,
        'classification_report': class_report,
        'saved_models': saved_models,
        'best_parameters': best_params
    }


def train_final_ensemble_model(X: pd.DataFrame, y_df: pd.DataFrame,
                              best_params: Dict[str, Any]) -> str:
    """Train final ensemble model on full dataset"""
    logger.info("Training final ensemble model on full dataset...")

    # Prepare final parameters
    final_params = best_params.copy()
    final_params.update({
        'objective': 'multi:softprob',
        'num_class': 3,
        'eval_metric': 'mlogloss',
        'tree_method': 'hist',
        'random_state': CONFIG['random_state'],
        'n_jobs': -1,
    })

    # Map labels
    y_mapped = y_df['label'].map({-1: 0, 0: 1, 1: 2})

    # Train on full dataset
    final_model = xgb.XGBClassifier(**final_params)
    # Safe drop of 'symbol' column
    X_numeric = X.drop(columns=['symbol'], errors='ignore')
    final_model.fit(X_numeric, y_mapped)

    # Save final model
    final_model_path = MODEL_DIR / "enhanced_final_ensemble_model_v6.joblib"
    joblib.dump(final_model, final_model_path)

    logger.success(f"Final ensemble model saved to: {final_model_path}")
    return str(final_model_path)


def main():
    """Main function to run the full optimization and validation pipeline."""
    logger.info(f"=== Starting Comprehensive Model Optimization (v6) ===")
    logger.info(f"Configuration: {json.dumps(CONFIG, indent=2)}")

    # --- 1. Load Data ---
    data_dir = project_root / "data" / "processed"
    X = pd.read_parquet(data_dir / "X_full_v3.parquet")
    y_df = pd.read_parquet(data_dir / "y_full_v3.parquet")

    # --- 2. Align and Prepare Data ---
    common_idx = X.index.intersection(y_df.index)
    X_aligned, y_df_aligned = X.loc[common_idx], y_df.loc[common_idx]
    t1 = y_df_aligned['t1']

    X_for_split = X_aligned.reset_index(drop=True)
    y_df_for_split = y_df_aligned.reset_index(drop=True)
    y_for_split = y_df_for_split['label']

    logger.info(f"Aligned datasets. Number of samples: {len(X_for_split)}")
    logger.info(f"Label distribution: {y_for_split.value_counts().sort_index().to_dict()}")

    # --- 3. Create CV Splits ---
    cv = PurgedStratifiedKFold(
        n_splits=CONFIG['n_splits'], 
        samples_info_sets=t1, 
        pct_embargo=CONFIG['pct_embargo'],
        min_samples_per_class=CONFIG['min_samples_per_class']
    )
    cv_splits = list(cv.split(X_for_split, y_for_split))

    # --- 4. Run Optuna Optimization ---
    sampler = TPESampler(seed=CONFIG['random_state'])
    study = optuna.create_study(direction="maximize", study_name="financial_ml_model_v6", sampler=sampler)
    try:
        study.optimize(
            lambda trial: objective(trial, X_for_split, y_df_for_split, cv_splits),
            n_trials=CONFIG['n_trials'],
            n_jobs=CONFIG['n_jobs_optuna']
        )
    except KeyboardInterrupt:
        logger.warning("Optimization interrupted by user. Saving partial results.")
    finally:
        # --- 5. Save Results ---
        logger.success(f"Optimization completed! Best score: {study.best_value or 'N/A'}")
        save_comprehensive_results(study, MODEL_DIR)

    # --- 6. Final Validation ---
    if study.best_trial:
        validation_results = final_model_validation(study, X_for_split, y_df_for_split, cv_splits)
        with open(MODEL_DIR / "final_validation_results_v6.json", "w") as f:
            json.dump(validation_results, f, indent=4, default=str)

    # --- 7. Train Final Model ---
    if study.best_trial:
        final_model_path = train_final_ensemble_model(X_for_split, y_df_for_split, study.best_params)
        logger.success(f"Final model saved to: {final_model_path}")
    else:
        logger.error("No successful trial found. Skipping final model training.")

    logger.success("=== Optimization Pipeline (v6) Completed Successfully ===")
    logger.success(f"Final model saved to: {final_model_path}")

if __name__ == "__main__":
    main()
